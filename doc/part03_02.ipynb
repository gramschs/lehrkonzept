{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a437e28-877f-458e-a22f-9af74b3603d3",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76fcc8",
   "metadata": {},
   "source": [
    "# Demo Datengestütztes Prozessmangement\n",
    "\n",
    "Das Perzeptron ist die Grundlage neuronaler Netze.\n",
    "\n",
    "## Das Perzeptron\n",
    "\n",
    "1943 haben die Forscher Warren McCulloch und Walter Pitts das erste Modell einer vereinfachten Hirnzelle präsentiert. Zu Ehren der beiden Forscher heißt dieses Modell MCP-Neuron. Darauf aufbauend publizierte Frank Rosenblatt 1957 seine Idee einer Lernregel für das künstliche Neuron. Das sogenannte Perzeptron bildet bis heute die Grundlage der künstlichen neuronalen Netze. Inspiriert wurden die Forscher dabei durch den Aufbau des Gehirns und der Verknüpfung der Nervenzellen.\n",
    "\n",
    "```{figure} pics/neuron_wikipedia.png\n",
    "---\n",
    "width: 600px\n",
    "name: fig_neuron_wikipedia\n",
    "---\n",
    "Schematische Darstellung einer Nervenzelle \n",
    "\n",
    "([Quelle:](https://de.wikipedia.org/wiki/Künstliches_Neuron#/media/Datei:Neuron_(deutsch)-1.svg) \"Schematische Darstellung einer Nervenzelle\" von Autor unbekannt. Lizenz: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/))\n",
    "```\n",
    "\n",
    "Elektrische und chemische Eingabesignale kommen bei den Dendriten an und laufen im Zellkörper zusammen. Sobald ein bestimmter Schwellwert überschritten wird, wird ein Ausgabesignal erzeugt und über das Axon weitergeleitet. Mehr Details zu Nervenzellen finden Sie bei Wikipedia: https://de.wikipedia.org/wiki/Nervenzelle.\n",
    "\n",
    "\n",
    "## Eine mathematische Ungleichung ersetzt das logische Oder\n",
    "\n",
    "Das einfachste künstliche Neuron besteht aus zwei Inputs und einem Output. Dabei sind für die beiden Inputs nur zwei Zustände zugelassen und auch der Output besteht nur aus zwei verschiedenen Zuständen. In der Sprache des maschinellen Lernens liegt also eine **binäre Klassifikationsaufgabe** innerhalb des **Supervised Learnings** vor.\n",
    "\n",
    "Beispiel:\n",
    "* Input 1: Es regnet oder es regnet nicht.\n",
    "* Input 2: Der Rasensprenger ist an oder nicht.\n",
    "* Output: Der Rasen wird nass oder nicht.\n",
    "\n",
    "Den Zusammenhang zwischen Regen, Rasensprenger und nassem Rasen können wir in einer Tabelle abbilden:\n",
    "\n",
    "Regnet es? | Ist Sprenger an? | Wird Rasen nass?\n",
    "-----------|------------------|-----------------\n",
    "nein       | nein             | nein\n",
    "ja         | nein             | ja\n",
    "nein       | ja               | ja\n",
    "ja         | ja               | ja\n",
    "```{exercise}\n",
    ":label: exercise_08_01\n",
    "Schreiben Sie ein kurzes Python-Programm, das abfragt, ob es regnet und ob der Rasensprenger eingeschaltet ist. Dann soll der Python-Interpreter ausgeben, ob der Rasen nass wird oder nicht. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70763ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geben Sie nach diesem Kommentar Ihren Code ein:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06767cf7",
   "metadata": {},
   "source": [
    "````{solution} exercise_08_01\n",
    ":label: solution_08_01\n",
    ":class: dropdown\n",
    "```python\n",
    "# Eingabe\n",
    "x1 = input('Regnet es (j/n)?')\n",
    "x2 = input('Ist der Rasensprenger eingeschaltet? (j/n)')\n",
    "\n",
    "# Verarbeitung\n",
    "y = (x1 == 'j') or (x2 == 'j')\n",
    "\n",
    "# Ausgabe\n",
    "if y == True:\n",
    "    print('Der Rasen wird nass.')\n",
    "else:\n",
    "    print('Der Rasen wird nicht nass.')\n",
    "```\n",
    "````\n",
    "Für das maschinelle Lernen müssen die Daten als Zahlen aufbereitet werde, damit die maschinellen Lernverfahren in der Lage sind, Muster in den Daten zu erlernen. Anstatt \"Regnet es? Nein.\" oder Variablen mit True/False setzen wir jetzt Zahlen ein. Die Inputklassen kürzen wir mit x1 für Regen und x2 für Rasensprenger ab. Die 1 steht für ja, die 0 für nein. Den Output bezeichnen wir mit y. Dann lautet die obige Tabelle für das \"Ist-der-Rasen-nass-Problem\":\n",
    "\n",
    "x1 | x2 | y\n",
    "---|----|---\n",
    "0  | 0  | 0\n",
    "1  | 0  | 1\n",
    "0  | 1  | 1\n",
    "1  | 1  | 1\n",
    "```{exercise}\n",
    ":label: exercise_08_02\n",
    "Schreiben Sie Ihren Programm-Code aus {ref}`exercise_08_01` um. Verwenden Sie Integer 0 und 1 für die Eingaben.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765fee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geben Sie nach diesem Kommentar Ihren Code ein:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ce866",
   "metadata": {},
   "source": [
    "````{solution} exercise_08_02\n",
    ":label: solution_08_02\n",
    ":class: dropdown\n",
    "```python\n",
    "# Eingabe\n",
    "x1 = int(input('Regnet es (ja = 1 | nein = 0)?'))\n",
    "x2 = int(input('Ist der Rasensprenger eingeschaltet? (ja = 1 | nein = 0)'))\n",
    "\n",
    "# Verarbeitung\n",
    "y = (x1 == 1) or (x2 == 1)\n",
    "\n",
    "# Ausgabe\n",
    "if y == True:\n",
    "    print('Der Rasen wird nass.')\n",
    "else:\n",
    "    print('Der Rasen wird nicht nass.')\n",
    "```\n",
    "````\n",
    "Als nächstes ersetzen wir das logische ODER durch ein mathematisches Konstrukt:\n",
    "Wenn die Ungleichung\n",
    "\n",
    "$$\\omega_1 x_1 + \\omega_2 x_2 \\geq \\theta$$\n",
    "\n",
    "erfüllt ist, dann ist $y = 1$ oder anders ausgedrückt, der Rasen wird nass. Und ansonsten ist $y = 0$, der Rasen wird nicht nass. Allerdings müssen wir noch die Gewichte $\\omega_1$ und $\\omega_2$ (in Englisch: weights) geschickt wählen. Die Zahl $\\theta$ ist der griechische Buchstabe Theta und steht als Abkürzung für den sogenannten Schwellenwert (in Englisch: threshold).\n",
    "\n",
    "Beispielsweise $\\omega_1 = 0.3$, $\\omega_2=0.3$ und $\\theta = 0.2$ passen:\n",
    "* $0.3\\cdot 0 + 0.3\\cdot 0 = 0.0 \\geq 0.2$ nicht erfüllt\n",
    "* $0.3\\cdot 1 + 0.3\\cdot 0 = 0.3 \\geq 0.2$ erfüllt\n",
    "* $0.3\\cdot 0 + 0.3\\cdot 1 = 0.3 \\geq 0.2$ erfüllt\n",
    "* $0.3\\cdot 1 + 0.3\\cdot 1 = 0.6 \\geq 0.2$ erfüllt\n",
    "\n",
    "\n",
    "```{exercise}\n",
    ":label: exercise_08_03\n",
    "Schreiben Sie Ihren Programm-Code aus {ref}`exercise_08_02` um. Ersetzen Sie das logische ODER durch die linke Seite der Ungleichung und vergleichen Sie anschließend mit $0.2$, um zu entscheiden, ob der Rasen nass wird oder nicht. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0124d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geben Sie nach diesem Kommentar Ihren Code ein:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e346c1",
   "metadata": {},
   "source": [
    "````{solution} exercise_08_03\n",
    ":label: solution_08_03\n",
    ":class: dropdown\n",
    "```python\n",
    "# Eingabe\n",
    "x1 = int(input('Regnet es (ja = 1 | nein = 0)?'))\n",
    "x2 = int(input('Ist der Rasensprenger eingeschaltet? (ja = 1 | nein = 0)'))\n",
    "\n",
    "# Verarbeitung\n",
    "y = 0.3 * x1 + 0.3 * x2\n",
    "\n",
    "# Ausgabe\n",
    "if y >= 0.2:\n",
    "    print('Der Rasen wird nass.')\n",
    "else:\n",
    "    print('Der Rasen wird nicht nass.')\n",
    "```\n",
    "````\n",
    "## Die Heaviside-Funktion ersetzt die Ungleichung \n",
    "\n",
    "Noch sind wir aber nicht fertig, denn auch die Frage \"Ist die Ungleichung erfüllt oder nicht?\" muss noch in eine mathematische Funktion umgeschrieben werden. Dazu subtrahieren wir zuerst auf beiden Seiten der Ungleichung den Schwellenwert $\\theta$:\n",
    "\n",
    "$$-\\theta + \\omega_1 x_1 + \\omega_2 x_2 \\geq 0.$$\n",
    "\n",
    "Damit haben wir jetzt nicht mehr einen Vergleich mit dem Schwellenwert, sondern müssen nur noch entscheiden, ob der Ausdruck $-\\theta + \\omega_1 x_1 + \\omega_2 x_2$ negativ oder positiv ist. Bei negativen Werten, soll $y = 0$ sein und bei positiven Werten (inklusive der Null) soll $y = 1$ sein. Dafür gibt es in der Mathematik eine passende Funktion, die sogenannte Heaviside-Funktion (manchmal auch Theta-, Stufen- oder Treppenfunktion genannt), siehe https://de.wikipedia.org/wiki/Heaviside-Funktion.\n",
    "\n",
    "```{figure} pics/heaviside_wikipedia.png\n",
    "---\n",
    "width: 600px\n",
    "name: fig_heaviside_wikipedia\n",
    "---\n",
    "Schaubild der Heaviside-Funktion\n",
    "\n",
    "([Quelle:](https://de.wikipedia.org/wiki/Heaviside-Funktion#/media/Datei:Heaviside.svg) \"Verlauf der Heaviside-Funktion auf $\\mathbb{R}$\" von Lennart Kudling. Lizenz: gemeinfrei)\n",
    "```\n",
    "Definiert ist die Heaviside-Funktion folgendermaßen:\n",
    "\n",
    "$$\\Phi(x) = \\begin{cases}0:&x<0\\\\1:&x\\geq 0\\end{cases}$$\n",
    "\n",
    "In dem Modul NumPy ist die Heaviside-Funktion schon hinterlegt, siehe \n",
    "> https://numpy.org/doc/stable/reference/generated/numpy.heaviside.html   \n",
    "\n",
    "```{exercise}\n",
    ":label: exercise_08_04\n",
    "Visualisieren Sie die Heaviside-Funktion für das Intervall $[-3,3]$ mit 101 Punkten. Setzen Sie das zweite Argument einmal auf 0 und einmal auf 2. Was bewirkt das zweite Argument? Sehen Sie einen Unterschied in der Visualisierung? Erhöhen Sie auch die Anzahl der Punkte im Intervall. Wählen Sie dabei immer eine ungerade Anzahl, damit die 0 dabei ist.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geben Sie nach diesem Kommentar Ihren Code ein:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269bc88e",
   "metadata": {},
   "source": [
    "````{solution} exercise_08_04\n",
    ":label: solution_08_04\n",
    ":class: dropdown\n",
    "```python\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-3,3, 101)\n",
    "y0 = np.heaviside(x, 0)     # an der Stelle x=0 ist y=0\n",
    "y1 = np.heaviside(x, 2)     # an der Stelle x=0 ist y=2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,y0)\n",
    "ax.plot(x,y1)\n",
    "ax.grid()\n",
    "ax.set_title('Heaviside-Funktion');\n",
    "```\n",
    "````\n",
    "\n",
    "Mit der Heaviside-Funktion können wir nun den Vergleich in der Programmverzweigung mit $0.2$ durch eine direkte Berechnung ersetzen. Schauen Sie sich im folgenden Programm-Code an, wie wir jetzt ohne logisches Oder und ohne Programmverzweigung if-else auskommen:\n",
    "```python\n",
    "# Import der notwendigen Module\n",
    "import numpy as np\n",
    "\n",
    "# Eingabe\n",
    "x1 = int(input('Regnet es (ja = 1 | nein = 0)?'))\n",
    "x2 = int(input('Ist der Rasensprenger eingeschaltet? (ja = 1 | nein = 0)'))\n",
    "\n",
    "# Verarbeitung\n",
    "y = np.heaviside(0.3 * x1 + 0.3 * x2, 1.0)\n",
    "\n",
    "# Ausgabe\n",
    "ergebnis_als_text = ['Der Rasen wird nass.', 'Der Rasen wird nicht nass.']\n",
    "print(ergebnis_als_text[int(y)])\n",
    "``` \n",
    "## Das Perzeptron mit mehreren Eingabewerten\n",
    "\n",
    "Das Perzeptron für zwei Eingabewerte lässt sich in sehr natürlicher Weise auf viele Eingabewerte verallgemeinern.\n",
    "\n",
    "Wenn wir nicht nur zwei, sondern $n$ Eingabewerte $x_i$ haben, brauchen wir entsprechend auch $n$ Gewichte $\\omega_i$. Die Eingabewerte können wir in einem Spaltenvektor zusammenfassen, also\n",
    "\n",
    "$$\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}.$$\n",
    "\n",
    "Auch die Gewichte können wir in einem Spaltenvektor zusammenfassen, also\n",
    "\n",
    "$$\\boldsymbol{\\omega} = \\begin{pmatrix} \\omega_1 \\\\ \\omega_2 \\\\ \\vdots \\\\ \\omega_n\\end{pmatrix}$$\n",
    "\n",
    "schreiben. Nun lässt sich die Ungleichung recht einfach durch das Skalarprodukt abkürzen:\n",
    "\n",
    "$$\\boldsymbol{\\omega}^{T}\\mathbf{x} = \\omega_1 x_1 + \\omega_2 x_2 + \\ldots + \\omega_n x_n \\geq \\theta.$$\n",
    "\n",
    "Wie bei dem Perzeptron mit zwei Eingängen wird der Schwellenwert $\\theta$ durch Subtraktion auf die linke Seite gebracht. Wenn wir jetzt bei dem Vektor $\\boldsymbol{\\omega}$ mit den Gewichten vorne den Vektor um das Element $\\omega_0 = -\\theta$ ergänzen und den Vektor $\\mathbf{x}$ mit $x_0 = 1$ erweitern, dann erhalten wir\n",
    "\n",
    "$$\\boldsymbol{\\omega}^{T}\\mathbf{x} = -\\theta \\cdot 1+ \\omega_1 x_1 + + \\omega_2 x_2 + \\ldots + \\omega_n x_n \\geq 0.$$\n",
    "\n",
    "Genaugenommen hätten wir jetzt natürlich für die Vektoren $\\boldsymbol{\\omega}$ und $\\mathbf{x}$ neue Bezeichnungen einführen müssen, aber ab sofort gehen wir immer davon aus, dass die mit dem negativen Schwellenwert $-\\theta$ und $1$ erweiterten Vektoren gemeint sind. Der negative Schwellenwert wird übrigens in der ML-Community **Bias** oder **Bias-Einheit (Bias Unit)** genannt. \n",
    "\n",
    "Um jetzt klassfizieren zu können, wird auf die gewichtete Summe $\\boldsymbol{\\omega}^{T}\\mathbf{x}$ die Heaviside-Funktion angewendet. Manchmal wird anstatt der Heaviside-Funktion auch die Signum-Funktion verwendet. Im Folgenden nennen wir die Funktion, die auf die gewichtete Summe angewendet wird, **Aktivierungsfunktion**.\n",
    "\n",
    "````{prf:definition}\n",
    ":label: def:08:01:01\n",
    "Das Perzeptron ist ein künstliches Neuron mit Gewichten $\\boldsymbol{\\omega}$ und einem Schwellenwert $\\theta$. Das Perzeptron berechnet eine gewichtete Summe der Inputs $\\mathbf{x}\\in\\mathbb{R}^n$ und wendet dann eine Aktivierungsfunktion $\\Phi$ an, um den Output $y$ zu berechnen:\n",
    "\n",
    "$$y = \\Phi(\\boldsymbol{\\omega}^{T}\\mathbf{x}) = \\Phi(-\\theta + \\omega_1 x_1 + \\ldots + \\omega_n x_n).$$\n",
    "\n",
    "````\n",
    "Eine typische Visualisierung des Perzeptrons ist in der folgenden Abbildung {ref}`fig_perzeptron` gezeigt. Die Inputs und der Bias werden durch Kreise symbolisiert. Die Multiplikation der Inputs $x_i$ wird durch Kanten dargestellt, die mit den Gewichten $\\omega_i$ beschriftet sind. Die einzelnen Summanden $\\omega_i x_i$ treffen sich sozusagen im mittleren Kreis, wo auf die gewichtete Summe dann eine Aktivierungsfunktion angewendet wird. Das Ergebnis, der Output $y$ wird dann berechnet und wiederum als Kreis gezeichnet. \n",
    "\n",
    "```{figure} pics/perzeptron.png\n",
    "---\n",
    "width: 600px\n",
    "name: fig_perzeptron\n",
    "---\n",
    "Schematische Darstellung eines Perzeptrons\n",
    "```\n",
    "\n",
    "## Hebbsche Regel\n",
    "\n",
    "Kaum zu glauben, aber die Idee zum Lernen der Gewichte eines Perzeptrons stammt nicht von Informatiker:innen, sondern von einem Psychologen namens [Donald Olding Hebb](https://de.wikipedia.org/wiki/Donald_O._Hebb). Im Englischen wird seine Arbeit meist durch das Zitat \n",
    "\n",
    ">\"what fires together, wires together\" \n",
    "\n",
    "kurz zusammengefasst. Hebb hat die Veränderung der synaptischen Übertragung von Neuronen untersucht und dabei festgestellt, dass je häufiger zwei Neuronen gemeinsam aktiv sind, desto eher werden die beiden aufeinander reagieren.\n",
    "\n",
    "Die Hebbsche Regel wird beim maschinellen Lernen dadurch umgesetzt, dass der Lernprozess mit zufälligen Gewichten startet und dann der prognostizierte Output mit dem echten Output verglichen wird. Je nachdem, ob der echte Output erreicht wurde oder nicht, werden nun die Gewichte und damit der Einfluss eines einzelnen Inputs verstärkt oder nicht. Dieser Prozess - Vergleichen und Abändern der Gewichte - wird solange wiederholt, bis die passenden Gewichte gefunden sind. \n",
    "\n",
    "Als Formel wird die Lernregel folgendermaßen formuliert:\n",
    "\n",
    "$$\\omega_i^{\\text{neu}} = \\omega_i^{\\text{aktuell}} + \\alpha \\cdot(y - \\hat{y}^{\\text{aktuell}}) \\cdot x_i.$$\n",
    "\n",
    "Dabei gehen wir davon aus, dass das Perzeptron $m$ Inputs mit der Bezeichnung $x_i$ und eine Bias-Einheit $x_0=1$ hat. Der Output ist hier mit $y$ gekennzeichnet. Wir berechnen nun, welchen Output $\\hat{y}^{\\text{aktuell}}$ das Perzeptron mit den aktuellen Gewichten $(\\omega_0^{\\text{aktuell}}, \\omega_1^{\\text{aktuell}}, \\ldots, \\omega_m^{\\text{aktuell}})$ prognostizieren würde. Dann vergleichen wir diesen Output mit dem echten Output $y$. \n",
    "* Wenn jetzt der echte Output größer ist als der Wert, den unser Perzeptron prognostiziert, dann ist $y - \\hat{y}^{\\text{aktuell}} > 0$. Indem wir nun zu den alten Gewichten den Term $ \\alpha \\cdot(y - \\hat{y}^{\\text{aktuell}})$ addieren, verstärken wir die alten Gewichte. Dabei berücksichtigen wir, ob der Input überhaupt einen Beitrag zum Output liefert, indem wir zusätzlich mit $x_i$ multiplizieren. Ist nämlich der Input $x_i=0$, wird so nichts an den Gewichten geändert. \n",
    "* Ist jedoch der echte Output kleiner als der prognostizierte Output, dann ist $y - \\hat{y}^{\\text{aktuell}} < 0$. Daher werden nun die alten Gewichte durch die Addition des negativen Terms $\\alpha \\cdot(y - \\hat{y}^{\\text{aktuell}})\\cdot x_i$ abgeschwächt. \n",
    "\n",
    "Damit die Schritte zwischen der Abschwächung und der Verstärkung nicht zu groß werden, werden sie noch mit einem Vorfaktor $\\alpha$ multipliziert, der zwischen 0 und 1 liegt. Ein typischer Wert von $\\alpha$ ist $0.0001$. Dieser Vorfaktor $\\alpha$ wird **Lernrate** genannt.\n",
    "\n",
    "\n",
    "## Perzeptron-Training am Beispiel des logischen ODER\n",
    "\n",
    "Das logische Oder ist bereits durch die Angabe der folgenden vier Datensätzen komplett definiert:\n",
    "        \n",
    "x0 | x1 | x2 | y\n",
    "---|---|----|---\n",
    "1  | 0 | 0  | 0\n",
    "1  | 0 | 1  | 1\n",
    "1  | 1 | 0  | 1\n",
    "1  | 1 | 1  | 1\n",
    "\n",
    "Im Folgenden wird das Training eines Perzeptrons Schritt für Schritt vorgerechnet.\n",
    "\n",
    "### Schritt 1: Initialisierung der Gewichte und der Lernrate\n",
    "\n",
    "Wir brauchen für die drei Inputs drei Gewichte und setzen diese drei Gewichte jeweils auf 0. Wir sammeln die Gewichte als Vektor, also\n",
    "\n",
    "$$\\vec{\\omega}^{T} = (\\omega_0, \\omega_1, \\omega_2) = (0, 0, 0).$$\n",
    "\n",
    "Darüber hinaus müssen wir uns für eine Lernrate $\\alpha$ entscheiden. Obwohl normalerweise ein kleiner (aber positiver) Wert gewählt wird, setzen wir der Einfachheit halber die Lernrate auf 1, also $\\alpha = 1$.\n",
    "\n",
    "### Schritt 2: Berechnung des Outputs und ggf. Anpassung der Gewichte\n",
    "\n",
    "Wir setzen nun solange nacheinander den ersten, zweiten, dritten und vierten Trainingsdatensatz in die Berechnungsvorschrift unseres Perzeptrons ein, bis die Gewichte für alle vier Trainingsdatensätze zu einer korrekten Prognose führen. Zur Erinnerung, wir berechnen den aktuellen Output des Perzeptrons mit der Formel\n",
    "\n",
    "$$\\hat{y}^{aktuell} = \\Phi(\\boldsymbol{\\omega}^{T}\\mathbf{x}) = \\Phi(\\omega_0 x_0 + \\omega_1 x_1 + \\omega_2 x_2).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7229fc8",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "SVG('pics/part08_training_perzeptron.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c44f9",
   "metadata": {},
   "source": [
    "### Schritt 3: Terminierung\n",
    "\n",
    "Die letzten vier Iterationen mit den Gewichten $(-1,1,1)$ prognostizierten\n",
    "jeweils das richtige Ergebnis. Daher können wir nun mit den Iterationen stoppen.\n",
    "\n",
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "In diesem Kapitel haben wir die Theorie des Perzeptrons kennengelernt. Als\n",
    "nächstes beschäftigen wir uns damit, wie wir mit dem in Scikit-Learn\n",
    "implementierten Perzeptron handgeschriebene Ziffern klassifizieren."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
